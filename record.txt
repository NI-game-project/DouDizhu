experiment 2 a2c: actor_critic v4_n was trained for 2000 episodes with discounted reward-normalizaton: learning_rate=0.00005 and softmax function. doesnt learn anything

experiment 3 a2c: actor_critic v4_n was trained for 1500 episodes with discounted reward-normalizaton learning_rate=0.00005 and without softmax deleted

experiment 3 a2c: actor_critic v4 was trained for 1500 episodes with discounted reward-normalizaton learning_rate=0.001 and with softmax
experiment 4 a2c: exp 3 was taken and trained 500 episodes more with learning_rate = 0.0001
experiment 5 ac2: exp4 was taken and trained for 1000 episodes with learning reate = 0.0005

experiment ac b0 = 1500 episodes with discounted reward-normalizaton: learning_rate=0.0001 and ppo and Adam
experiment ac c0 = 1500 episodes without discounted reward, learning rate 0.0001 and ppo with RMSprop
experiment ac d0 = 1500 episodes without discounted reward, learning rate 0.00005 and ppo with RMSprop
experiment ac d0 = the one from d0 but 5000 more episodes without discounted reward, learning rate 0.00005 and ppo with RMSprop instead of random, rule based is plotted
experiment ac d2 = the one from d0 but 10000 more episodes without discounted reward, learning rate 0.00001 and ppo with RMSprop instead of random, rule based is plottedlearning_rate=0.00001


experiment dqn 1024 = 10000 episodes with larger network and without the predict function and learning rate 0.0001 start: 13:07, ende 17:37

experiment hypernetwork 1:  trained with learnining rate 0.0001 and 5000 episodes, after  2000 episodes always at the lower edge
experiment hypernetwork 2: trained with learnining rate 0.00001 and 5000 episodes, not converging at all after 2000 episodes
experiment hypernetwork 3: models/hypernetwork/a1.h5 trained with learnining rate 0.00005 and 2000 episodes

experiment hypernetwork 4: different from the first three becuase now the embedding vector for the weight generating part is 10 instead of 5
                            learing rate is again 0.00005 and 15.000 episodes also number of epochs has been doubeld to 10, but batch size remains at 8



experiment dqn e1 : trained against random agents for 5000 episodes with learning_rate=0.0001 and two 512 Dense layers + output layer the file is in the simple
                    dqn experiments folder
experiment dqn e2 : trained e1 for 5000 episodes but now with 0.0001 lr 
experiment dqn e3 : trained e2 again for 5000 episdodes with 0.0001 lr
experiment dqn e4 : trained e3 again for 5000 episdodes with 0.0001 lr
experiment dqn e5 : now train it against rule based agents for 5000 episdodes with 0.000s1 lr
experiment dqn e6 : now train it against other agents but evaluate against rule based for 5000 episdodes with 0.00005 lr



experiment genetic b1 : populationsize = 50 , generations = 100, trained against random agents, epsilon 0.002
experiment genetic c2 : populationsize = 50 , generations = 100, trained against random agents, epsilon 0.0005

experiment a2c s1 = 5000 episodes without discounted reward, learning rate 0.0001 and ppo with RMSprop

experiment genetic c1: populationsize = 50 , generations = 300, trained against random agents, epsilon 0.0005
experiment genetic d1: populationsize = 50 , generations = 300, trained the c1 elite against rule_based_agents, epsilon 0.0005
experiment genetic d2: populationsize = 50 , generations = 500, trained the d1 elite against rule_based_agents, epsilon 0.0001
experiment genetic d3: populationsize = 50 , generations = 300, trained the d2 elite against rule_based_agents, epsilon 0.00005

experiment genetic a2c a1: populationsize = 50 , generations = 100, trained against random, epsilon 0.0005 learning_rate=0.000025
experiment genetic a2c a2: populationsize = 50 , generations = 300, trained the a1 elite against rule_based_agents, epsilon 0.0005 learning_rate=0.000025
experiment genetic a2c a3: populationsize = 50 , generations = 600, trained the a2 elite against rule_based_agents, epsilon 0.0001 learning_rate=0.000025
